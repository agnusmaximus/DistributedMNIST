Cfg({
    "name" : "50_workers_3000_interval",      # Unique name for this specific configuration
    "key_name": "MaxLamKeyPair",          # Necessary to ssh into created instances

    # Cluster topology
    "n_masters" : 1,                      # Should always be 1
    "n_workers" : 49,
    "n_ps" : 1,
    "n_evaluators" : 1,                   # Continually validates the model on the validation data
    "num_replicas_to_aggregate" : "50",

    "method": "reserved",

    # Region speficiation
    "region" : "us-west-2",
    "availability_zone" : "us-west-2b",

    # Machine type - instance type configuration.
    "master_type" : "t2.large",
    "worker_type" : "t2.large",
    "ps_type" : "t2.large",
    "evaluator_type" : "t2.large",
    "image_id" : "ami-2306ba43",

    # Launch specifications
    "spot_price" : ".03",                 # Has to be a string

    # SSH configuration
    "ssh_username" : "ubuntu",            # For sshing. E.G: ssh ssh_username@hostname
    "path_to_keyfile" : "/Users/maxlam/Desktop/School/Fall2016/Research/DistributedSGD/DistributedSGD.pem",

    # NFS configuration
    # To set up these values, go to Services > ElasticFileSystem > Create new filesystem, and follow the directions.
    #"nfs_ip_address" : "172.31.3.173",         # us-west-2c
    #"nfs_ip_address" : "172.31.35.0",          # us-west-2a
    "nfs_ip_address" : "172.31.28.54",          # us-west-2b
    "nfs_mount_point" : "/home/ubuntu/inception_shared",       # NFS base dir
    "base_out_dir" : "%(nfs_mount_point)s/%(name)s", # Master writes checkpoints to this directory. Outfiles are written to this directory.

    "setup_commands" :
    [
        "sudo rm -rf %(base_out_dir)s",
        "mkdir %(base_out_dir)s",
    ],

    # Command specification
    # Master pre commands are run only by the master
    "master_pre_commands" :
    [
        "cd DistributedMNIST",
        "git fetch && git reset --hard origin/master",
    ],

    # Pre commands are run on every machine before the actual training.
    "pre_commands" :
    [
        "cd DistributedMNIST",
        "git fetch && git reset --hard origin/master",
    ],

    # Model configuration
    "batch_size" : "128",
    "initial_learning_rate" : ".0008",
    "learning_rate_decay_factor" : "1",
    "num_epochs_per_decay" : "1.0",

    # Train command specifies how the ps/workers execute tensorflow.
    # PS_HOSTS - special string replaced with actual list of ps hosts.
    # TASK_ID - special string replaced with actual task index.
    # JOB_NAME - special string replaced with actual job name.
    # WORKER_HOSTS - special string replaced with actual list of worker hosts
    # ROLE_ID - special string replaced with machine's identity (E.G: master, worker0, worker1, ps, etc)
    # %(...)s - Inserts self referential string value.
    "train_commands" :
    [
        "sudo python src/mnist_distributed_train.py "
        "--batch_size=%(batch_size)s "
        "--initial_learning_rate=%(initial_learning_rate)s "
        "--learning_rate_decay_factor=%(learning_rate_decay_factor)s "
        "--num_epochs_per_decay=%(num_epochs_per_decay)s "
        "--train_dir=%(base_out_dir)s/train_dir "
        "--worker_hosts='WORKER_HOSTS' "
	"--interval_method=true "
	"--interval_ms=3000 "
        "--ps_hosts='PS_HOSTS' "
        "--task_id=TASK_ID "
        "--num_replicas_to_aggregate=%(num_replicas_to_aggregate)s "
        "--job_name=JOB_NAME > %(base_out_dir)s/out_ROLE_ID 2>&1 &"
    ],

    # Commands to run on the evaluator
    "evaluate_commands" :
    [
        # Wait a bit
        "sleep 30",

        # Evaluation command
        "python src/mnist_eval.py "
        "--eval_dir=%(base_out_dir)s/eval_dir "
        "--checkpoint_dir=%(base_out_dir)s/train_dir "
        "> %(base_out_dir)s/out_evaluator 2>&1 &",

        # Tensorboard command
        "python /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/tensorboard.py "
        " --logdir=%(base_out_dir)s/train_dir/ "
        #" --logdir=%(base_out_dir)s/eval_dir/ "
        "> %(base_out_dir)s/out_evaluator_tensorboard 2>&1 &"
    ],
})
